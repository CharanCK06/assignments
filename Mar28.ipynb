{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333b9aa-2403-4d6c-a40d-0bf9ebd01deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "'''\n",
    "Ridge Regression is a regularization technique used in linear regression to prevent overfitting and improve the \n",
    "generalization performance of the model. It achieves this by adding a penalty term to the ordinary least squares \n",
    "(OLS) regression cost function.\n",
    "\n",
    "OLS regression aims to find the line (or hyperplane) that minimizes the sum of the squared differences between the \n",
    "predicted and actual target values. It directly fits the data without any constraints on the coefficients.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdcdce-04bd-4470-a23e-532f962fec92",
   "metadata": {},
   "outputs": [],
   "source": [
    " #2\n",
    "'''\n",
    "Linearity: The relationship between the predictor variables (features) and the response variable (target) is \n",
    "assumed to be linear. Ridge Regression works with the same linear relationship as OLS regression.\n",
    "\n",
    "Independence: The residuals (the differences between actual and predicted values) should be independent of each \n",
    "other. This assumption implies that the observations are not correlated.\n",
    "\n",
    "No Multicollinearity: The predictor variables should not be highly correlated with each other. Ridge Regression is \n",
    "effective at mitigating the impact of multicollinearity, but it's still important to address multicollinearity to \n",
    "ensure reliable coefficient estimates.\n",
    "\n",
    "Fixed Sample Size: The assumption is that the sample size is fixed, and not tending towards infinity.\n",
    "Large sample sizes are typically more robust against deviations from assumptions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7739b-11b6-4aaf-9ec1-e4acf4f0ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "'''\n",
    "Grid Search CV\n",
    "Randomized search cv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bc47c-c571-42d1-aaa4-55a5963f76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "'''\n",
    "Ridge Regression can be used for feature selection, but it does so in a different way compared to techniques like \n",
    "Lasso Regression. While Ridge does not drive coefficients exactly to zero like Lasso, it can still indirectly \n",
    "contribute to feature selection by shrinking the coefficients of less important features towards zero.\n",
    "\n",
    "Ridge Regression adds a penalty term based on the squared values of coefficients to the linear regression cost \n",
    "function. This penalty discourages large coefficient values. As 位 (the regularization parameter) increases, the \n",
    "effect of the penalty becomes stronger, causing the coefficients to shrink. The stronger the regularization, the \n",
    "closer the coefficients get to zero.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be661b6b-9303-46e6-bd10-c1aed62a189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "'''\n",
    "Ridge's penalty term, which is based on the squared magnitudes of coefficients, shrinks the coefficients towards \n",
    "zero. This effectively reduces the impact of highly correlated features, helping to stabilize the coefficient \n",
    "estimates.\n",
    "\n",
    "While Ridge Regression does not eliminate multicollinearity, it can effectively reduce its impact by preventing the\n",
    "model from relying heavily on a single correlated feature.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f05fbf-2720-4c4e-bb8b-efc170e2a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "'''\n",
    "Continuous Independent Variables:\n",
    "Ridge Regression naturally handles continuous independent variables, just like ordinary least squares (OLS) \n",
    "regression.\n",
    "The penalty term in Ridge is applied to the squared magnitudes of the coefficients, regardless of the \n",
    "nature of the independent variables.\n",
    "Continuous variables do not require any special treatment when using Ridge.\n",
    "\n",
    "Categorical Independent Variables:\n",
    "Encoding categorical variables\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fab4d-3eea-40e3-b7a1-84d241a90dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "'''\n",
    "Magnitude: The magnitude of the Ridge coefficients is influenced by the value of the regularization parameter 位.\n",
    "            As 位 increases, the magnitude of the coefficients tends to decrease towards zero.\n",
    "\n",
    "Sign: The sign of the Ridge coefficients indicates the direction of the relationship between the predictor variable\n",
    "    and the response variable, just like in OLS.\n",
    "\n",
    "Relative Importance: The relative importance of the Ridge coefficients is still informative. Larger magnitude \n",
    "    coefficients have a stronger impact on the response variable compared to smaller magnitude coefficients.\n",
    "\n",
    "Feature Impact: Positive coefficients indicate that an increase in the predictor variable is associated with an \n",
    "increase in the response variable. Negative coefficients indicate the opposite. The larger the coefficient, the \n",
    "stronger the impact of that feature on the response variable.\n",
    "\n",
    "Comparison Across Features: You can compare the relative impact of features by comparing their magnitudes, but keep\n",
    "in mind that the regularization effect might shrink some coefficients more than others.\n",
    "\n",
    "Normalization: Make sure your data is normalized before applying Ridge Regression to ensure the coefficients are \n",
    "on a similar scale. This allows you to compare the relative importance of features more accurately.\n",
    "\n",
    "Intercept: The intercept (bias term) in Ridge Regression is not regularized and remains largely unaffected by 位. \n",
    "Its interpretation remains similar to the intercept in OLS regression.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497fd83-17d6-438b-9f75-7f7a92b2d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "'''\n",
    "Yes, Ridge Regression can be used for time-series data analysis, but it requires some adaptations and considerations\n",
    "to address the temporal nature of the data. Time-series data presents unique challenges, such as autocorrelation \n",
    "and temporal dependencies, which need to be accounted for when applying Ridge Regression. \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
