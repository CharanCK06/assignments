{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b17ce-66cf-4af5-85bb-f98b314b2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "'''\n",
    "\n",
    "R-squared is a statistical measure used to assess the goodness of fit of a linear regression model.\n",
    "\n",
    "It provides insights into how well the independent variable(s) in the model explain the variability in the \n",
    "dependent variable. \n",
    "\n",
    "R2 = 1 - (Sum of squares of residuals/sum of squares of total)\n",
    "\n",
    "R-squared values range from 0 to 1. Here's what the values typically signify:\n",
    "R2 = 0, The model explains none of the variability in the dependent variable.\n",
    "R2 = 1, The model perfectly explains the variability in the dependent variable.\n",
    "\n",
    "A high R-squared value doesn't necessarily imply that the model is a good fit or that the relationships are causal.\n",
    "It's possible to achieve a high R-squared even with a model that's overfitting the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba96786-5e67-4f4f-8509-118b4aa5c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "'''\n",
    "\n",
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent\n",
    "variables in a linear regression model.\n",
    "\n",
    "It addresses one of the limitations of the regular R-squared, which tends to increase as you add more independent \n",
    "variables to the model, even if those variables are not significantly correlated to the dependant variable.\n",
    "\n",
    "R2 = 1 - [(1−R2)×(n−1)/(n−p−1)]\n",
    "\n",
    "R2 is the regular R-squared value.\n",
    "n is the number of observations.\n",
    "p is the number of independent variables in the model.\n",
    "\n",
    "Adjusted R-squared provides a more realistic assessment of the model's goodness of fit by accounting for the number\n",
    "of variables. It helps prevent overfitting\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01adb9-126f-4f40-8cfb-2bd958be9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "'''\n",
    "In cases where your regression model includes many independent variables, the adjusted R-squared helps you evaluate\n",
    "whether the added complexity due to additional variables is justified by the improvement in model fit.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398f116-aea5-4517-abbc-43eddded20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "'''\n",
    "MSE (Mean Squared Error):\n",
    " It calculates the average of the squared differences between the predicted values and the actual observed values.\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "MAE measures the average absolute differences between the predicted values and the actual observed values.\n",
    "It is less sensitive to outliers compared to MSE and RMSE.\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    " RMSE is a widely used metric that calculates the square root of the average of the squared differences between the \n",
    "  predicted values and the actual observed values. \n",
    " It provides a measure of the average magnitude of the errors in the model's predictions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b977036-819e-4a18-8c31-b40bc6546eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "'''\n",
    "MSE:\n",
    "    Advantages:\n",
    "        1. Since it is a quadratic equation it can be differentiable\n",
    "        2. It has 1 local and global minima\n",
    "    Disadvantages:\n",
    "        1. It is not robust to outliers.\n",
    "        2. Its values are not in the same units as the dependent variable.\n",
    "\n",
    "MAE:\n",
    "    Advnatages:\n",
    "        1.It is robust to outliers.\n",
    "        2.Its values are in the same unit as the dependent variable.\n",
    "    Disadvantages:\n",
    "        1.Convergence takes time.\n",
    "        2.Optimization is complex.\n",
    "    \n",
    "RMSE:\n",
    "    Advantages:\n",
    "        1. Its values are in the same unit as the dependent variable.\n",
    "        2. It can be differentiable.\n",
    "    Disadvantages:\n",
    "        1. It is not robust to outliers\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20083ab-73a8-4e67-b9fe-caddf276190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "'''\n",
    "Lasso regularization adds a penalty term to the standard linear regression cost function. \n",
    "- This penalty term is proportional to the absolute values of the coefficients of the model's features. \n",
    "- The goal of Lasso is to encourage the model to shrink the coefficients of less important features all the way to \n",
    "  zero, effectively performing feature selection by eliminating irrelevant features.\n",
    "  \n",
    "The main difference between Lasso and Ridge regularization lies in the type of penalty applied to the coefficients. \n",
    "Lasso uses the absolute values of coefficients, leading to sparse models, while Ridge uses the squared values of \n",
    "coefficients, shrinking but not necessarily eliminating any of them.\n",
    "\n",
    "If you want a model that is easy to interpret with a reduced set of features, Lasso can automatically identify and \n",
    "select the most important features, resulting in a simpler and more understandable model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe669f2-cae9-4b10-9ff9-8fd7f9b85408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "'''\n",
    "Regularized linear models, such as Lasso and Ridge regression, help prevent overfitting in machine learning by \n",
    "adding a penalty term to the standard linear regression cost function. \n",
    "This penalty term discourages the model from fitting the training data too closely and instead encourages it to \n",
    "generalize well to new, unseen data. \n",
    "The penalty term achieves this by constraining the magnitude of the model's coefficients.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ed2f7-d799-4f4b-9d65-ae149c904da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "'''\n",
    "Loss of Interpretability: Regularization techniques can shrink coefficients towards zero or eliminate them entirely.\n",
    "While this can lead to simpler models, it might also result in a loss of interpretability. \n",
    "\n",
    "\n",
    "Feature Scaling: Regularized linear models are sensitive to the scale of the features. \n",
    "If features are not properly scaled, the regularization term might disproportionately penalize certain features, \n",
    "leading to biased coefficients.\n",
    "\n",
    "Model Complexity and Bias-Variance Trade-off: Regularization can prevent the model from fitting the training data \n",
    "too closely, reducing overfitting. \n",
    "\n",
    "Selection of Hyperparameters: Regularized linear models have hyperparameters, such as the regularization strength\n",
    "(λ) in Lasso and Ridge, that need to be tuned. Selecting the appropriate hyperparameters can be a challenging task \n",
    "and might require cross-validation or other tuning techniques.\n",
    "\n",
    "High-Dimensional Data: While Lasso can perform feature selection by setting some coefficients to zero, it might \n",
    "struggle in cases where the number of features is much larger than the number of observations.\n",
    "\n",
    "Multicollinearity Handling: While Ridge regularization is effective at handling multicollinearity, Lasso might \n",
    "arbitrarily select one feature from a group of highly correlated features and drive the coefficients of the others \n",
    "to zero. This can lead to instability in model selection.\n",
    "\n",
    "Non-Linear Relationships: Regularized linear models assume a linear relationship between features and the target \n",
    "variable. \n",
    "If the true relationship is nonlinear, these models might not capture the underlying patterns effectively, \n",
    "requiring more complex techniques like polynomial regression or non-linear models.\n",
    "\n",
    "Outliers: Regularized models can be sensitive to outliers, as the penalty terms are based on the magnitude of the \n",
    "coefficients. Outliers can disproportionately affect the regularization process and lead to suboptimal solutions.\n",
    "\n",
    "Data Transformation: Regularization does not inherently handle skewed or non-normal data distributions well. \n",
    "Data transformation techniques might be needed to achieve better model performance.\n",
    "\n",
    "Alternative Regularization Techniques: Regularized linear models might not always be the best choice when other \n",
    "regularization techniques, like Elastic Net (which combines Lasso and Ridge), could provide a better balance \n",
    "between feature selection and coefficient stability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f52386-eead-4669-8efa-9530b9742b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "'''\n",
    "MAE of 8 is chosen over model that has rsme of 10.\n",
    "MAE  provides a more balanced assessment of overall prediction accuracy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706895d7-0423-4d64-91fb-3811f743af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "'''\n",
    "To determine which model is better, you would typically assess their performance on a validation or test dataset \n",
    "using appropriate evaluation metrics such as RMSE, MAE, or others relevant to your problem. Comparing these metrics\n",
    "can give you an indication of how well each model generalizes to new data.\n",
    "\n",
    "Keep in mind the following considerations:\n",
    "\n",
    "Feature Interpretability: If interpretability is crucial, Ridge might be preferred as it doesn't force coefficients\n",
    "to zero, allowing for easier interpretation of feature impacts. Lasso can lead to sparse models, which might be \n",
    "less intuitive.\n",
    "\n",
    "Feature Selection: If you suspect that some features are irrelevant or you prefer a more parsimonious model, \n",
    "Lasso's feature selection property could be advantageous.\n",
    "\n",
    "Correlated Features: If you have correlated features, Ridge is better at handling them since it doesn't force them \n",
    "to become exactly zero.\n",
    "\n",
    "Trade-offs: Lasso might be more sensitive to outliers than Ridge due to its absolute value penalty. Ridge might \n",
    "offer more balanced performance across various scenarios.\n",
    "\n",
    "Hyperparameter Tuning: The choice of regularization strength (λ) matters. It's important to perform proper \n",
    "hyperparameter tuning, perhaps via cross-validation, to select the optimal value for each model.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
