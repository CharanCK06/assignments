{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f95d92-e049-43a7-b93c-4794d8417c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "'''\n",
    "Simple Linear Regression:\n",
    "Simple linear regression involves a single independent variable (predictor) and a dependent variable.\n",
    "It aims to find a linear relationship between the predictor and the dependent variable. \n",
    "\n",
    "Equation: y = mx + c\n",
    "        y - dependent variable\n",
    "        x - independent variable\n",
    "        m - slope\n",
    "        c - intercept\n",
    "    ex: Prediction of student scores based on no.of.hours they studied.\n",
    "        Here Student score is dependent feature and no.of.hours studied is independent feature.\n",
    "        \n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression involves two or more independent variables (predictors) and a dependent variable.\n",
    "It extends the concept of simple linear regression to model the relationship between multiple predictors and the \n",
    "dependent variable. \n",
    "\n",
    "Equation: y = c + m1x1 + m2x2 \n",
    "\n",
    "    ex: Prediction of the House prices based on the factors like Dimensions,Location,Age etc\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27501f-ac94-4ba0-957e-02b27f9ba397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "'''\n",
    "Assumptions:\n",
    "\n",
    "i.Linearity: The relationship between the independent variables and the dependent variable is assumed to be linear.\n",
    "  - You can check this assumption by visually inspecting scatter plots of the dependent variable against each \n",
    "    independent variable.\n",
    "    \n",
    "ii.Independence: The observations in the dataset are assumed to be independent of each other.\n",
    "\n",
    "iii.Normality: The residuals are assumed to follow a normal distribution.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5ef18-1b56-49b0-a59f-5896849e23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "'''\n",
    "\n",
    "y = mx + c\n",
    "\n",
    "Intercept (c): The intercept represents the value of the dependent variable when all independent variables are set\n",
    "to zero. It indicates the baseline or starting point of the dependent variable when no predictors are present.\n",
    "\n",
    "Slope (m): The slope coefficient represents the change in the dependent variable associated with a one-unit \n",
    "increase in the independent variable, holding all other variables constant. It quantifies the rate or magnitude of\n",
    "change in the dependent variable per unit change in the independent variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b3138-964d-4d65-9a0d-d396e3f5a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "'''\n",
    "Gradient descent is an optimization algorithm used in machine learning to find the optimal values of the parameters\n",
    "(coefficients) of a model that minimizes a cost function.\n",
    "\n",
    "Cost Function: In machine learning, a cost function measures the discrepancy between the predicted values of the \n",
    "model and the actual values of the target variable. The goal is to minimize this cost function, indicating a better\n",
    "fit of the model to the data.\n",
    "\n",
    "Gradient: The gradient is a vector that represents the direction and magnitude of the steepest ascent or descent of\n",
    "a function at a specific point. It indicates the direction in which the function increases the most rapidly.\n",
    "\n",
    "Gradient Descent Process: The gradient descent algorithm starts with initial values for the parameters and \n",
    "calculates the gradient of the cost function with respect to each parameter. \n",
    "\n",
    "Learning Rate: The learning rate is a hyperparameter that controls the size of the steps taken during each \n",
    "iteration of gradient descent. It determines the speed at which the algorithm converges to the minimum.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77560154-985b-4ce1-a6fd-323ee86e04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "'''\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression involves two or more independent variables (predictors) and a dependent variable.\n",
    "It extends the concept of simple linear regression to model the relationship between multiple predictors and the \n",
    "dependent variable. \n",
    "\n",
    "Equation: y = c + m1x1 + m2x2 \n",
    "\n",
    "    ex: Prediction of the House prices based on the factors like Dimensions,Location,Age etc\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85334320-e03b-48dc-a808-9505e6fab90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "'''\n",
    "Multicollinearity exists when there is a strong linear relationship between two or more independent variables, \n",
    "making it difficult for the model to distinguish the individual effects of these variables on the dependent \n",
    "variable.\n",
    "\n",
    "There are several methods to detect multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation matrix among the independent variables. Correlation values close \n",
    "                    to +1 or -1 indicate strong linear relationships.\n",
    "\n",
    "VIF (Variance Inflation Factor): VIF measures the extent to which the variance of the estimated regression \n",
    "coefficient is increased due to multicollinearity. High VIF values (typically above 10) suggest\n",
    "multicollinearity.\n",
    "\n",
    "Tolerance: Tolerance is the reciprocal of VIF. Low tolerance values (below 0.2) indicate high multicollinearity.\n",
    "\n",
    "There are several strategies to address the issue:\n",
    "\n",
    "- Feature Selection: Remove one or more of the correlated variables from the model. This simplifies the model \n",
    "    and eliminates the multicollinearity problem.\n",
    "\n",
    "- Combine Variables: Create new variables by combining the correlated variables, reducing the multicollinearity \n",
    "    by creating a single composite variable.\n",
    "\n",
    "- Regularization Techniques: Use regularization methods like Ridge or Lasso regression. These techniques add \n",
    "    penalty terms to the regression equation, which can mitigate the impact of multicollinearity.\n",
    "\n",
    "- Collect More Data: Increasing the sample size can help mitigate the effects of multicollinearity by providing \n",
    "    a broader range of data points.\n",
    "\n",
    "- Domain Knowledge: Use domain knowledge to decide which variables are the most important and drop the redundant\n",
    "    ones.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17455b49-b328-4e7c-a8eb-409f4d815f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "'''\n",
    "Polynomial regression is a type of regression analysis that extends the linear regression model to capture \n",
    "nonlinear relationships between the independent variable(s) and the dependent variable.\n",
    "\n",
    "While linear regression assumes a linear relationship between the variables, polynomial regression allows for \n",
    "curves and bends in the relationship by introducing polynomial terms of higher degrees.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9cf1a-7850-4b64-ae35-20131ec1cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "'''\n",
    "Advantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "1.Capturing Nonlinear Relationships: Polynomial regression can model nonlinear relationships between variables, \n",
    " allowing it to fit data that doesn't follow a linear pattern. Linear regression is limited to modeling linear \n",
    " relationships.\n",
    "\n",
    "2.Flexible Fit: With higher-degree polynomial terms, polynomial regression can closely fit complex patterns in \n",
    " the data, which can be particularly useful when the underlying relationship is not simple.\n",
    "\n",
    "Situations to Prefer Polynomial Regression:\n",
    "\n",
    "Nonlinear Patterns: When you observe clear nonlinear patterns in the data, polynomial regression can be a better\n",
    " choice than linear regression to capture these relationships.\n",
    "\n",
    "Limited Data Range: If the data range is restricted and the relationship between variables is nonlinear within \n",
    " that range, polynomial regression might be more suitable.\n",
    "\n",
    "Visual Evidence: When visualizing the data reveals curvature or bends that a linear model can't capture, \n",
    " polynomial regression can provide a better fit.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
