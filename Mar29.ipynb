{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc35c5d-7508-41f2-839b-2951a5ac8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "'''\n",
    "\n",
    "Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator\" Regression, is a linear regression \n",
    "technique that incorporates regularization to improve the model's predictive performance and feature selection.\n",
    "\n",
    "It is particularly useful when dealing with datasets that have a large number of features (variables) and potential\n",
    "multicollinearity (correlations between predictor variables).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8405f-ee6a-463d-a524-bced05aaa23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "'''\n",
    "The main advantage of using Lasso Regression for feature selection is its ability to automatically identify and \n",
    "select the most relevant features from a larger set of predictors. This is achieved by forcing some of the \n",
    "coefficients associated with less important features to become exactly zero, effectively removing those features \n",
    "from the model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee97af8-e85e-4e0f-8f91-1064f4f2e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "'''\n",
    "Sign of the Coefficient:\n",
    "Just like in regular linear regression, the sign of a coefficient indicates the direction of the relationship \n",
    "between the predictor variable and the target variable. A positive coefficient indicates a positive correlation, \n",
    "meaning that an increase in the predictor's value leads to an increase in the predicted target value, and vice \n",
    "versa. A negative coefficient indicates a negative correlation.\n",
    "\n",
    "Magnitude of the Coefficient:\n",
    "The magnitude of a coefficient represents the strength of the relationship between the predictor and the target \n",
    "variable, holding other predictors constant. Larger absolute coefficient values indicate a stronger influence on \n",
    "the target variable.\n",
    "\n",
    "Coefficient Value Near Zero:\n",
    "In Lasso Regression, one of the main features is that some coefficients can be exactly zero. This means that the \n",
    "corresponding feature has been effectively excluded from the model due to its limited impact on the target variable.\n",
    "A coefficient of exactly zero indicates that the associated predictor has no effect on the predicted target value.\n",
    "\n",
    "Comparing Magnitudes:\n",
    "When comparing coefficients across different features, you need to consider the magnitudes relative to each other. \n",
    "Larger coefficients typically have a larger impact on the target variable. However, be cautious when comparing \n",
    "coefficients across features with different scales, as the scale of the predictor variable can influence the size \n",
    "of the coefficient.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3f5a0-f152-47bc-8571-f5e96c059974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "'''\n",
    "The lambda parameter controls the strength of the L1 regularization penalty in Lasso Regression. \n",
    "\n",
    "A larger value of 位 increases the amount of regularization, which leads to more coefficients being pushed towards \n",
    "zero, resulting in a sparser model. \n",
    "\n",
    "Conversely, a smaller value of 位 reduces the amount of regularization, allowing the model to retain more features \n",
    "with nonzero coefficients. \n",
    "\n",
    "The choice of 位 is critical, as it directly influences the trade-off between fitting the data well and keeping the \n",
    "model simple to prevent overfitting. \n",
    "\n",
    "Cross-validation techniques (such as k-fold cross-validation) can be used to determine the optimal value of 位 that \n",
    "yields the best performance on new, unseen data.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f25d2-211a-4ca9-960b-0c8b77dbc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "'''\n",
    "Polynomial Features:\n",
    "You can create polynomial features by raising the existing predictor variables to different powers. \n",
    "\n",
    "Interaction Features:\n",
    "Interaction features involve creating new features by multiplying pairs of existing features. For instance, \n",
    "if you have predictor variables \"x\" and \"y,\" you can add an interaction feature \"xy.\" This approach can help \n",
    "capture interactions and non-linear effects that are not directly represented by the original features.\n",
    "\n",
    "Transformations:\n",
    "Apply non-linear transformations to the existing features. Common transformations include logarithmic, exponential,\n",
    "and trigonometric functions. These transformations can help linearize relationships and make them more amenable to \n",
    "Lasso Regression.\n",
    "\n",
    "Basis Functions:\n",
    "Introduce basis functions that map the original features into a higher-dimensional space where they can exhibit \n",
    "non-linear behavior. These basis functions can be chosen based on domain knowledge or exploration.\n",
    "\n",
    "Kernel Trick:\n",
    "The kernel trick is commonly used in Support Vector Machines, but it can also be adapted for Lasso Regression. It \n",
    "involves mapping the original features into a higher-dimensional space using a kernel function, which allows the \n",
    "model to capture non-linear relationships.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580cacd4-5407-41b1-941e-8fd720a428b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "'''\n",
    "Ridge Regression adds an L2 regularization penalty to the linear regression objective function. The L2 penalty term\n",
    "is the sum of the squared magnitudes of the coefficients, multiplied by a hyperparameter (lambda or alpha). It \n",
    "encourages the coefficients to be small, but it doesn't force them to be exactly zero.\n",
    "\n",
    "Lasso Regression, on the other hand, uses an L1 regularization penalty. The L1 penalty is the sum of the absolute \n",
    "values of the coefficients, multiplied by the hyperparameter. Unlike Ridge, Lasso has the ability to drive some \n",
    "coefficients to exactly zero, effectively performing feature selection.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9136ce0-1300-439d-8b33-f88d1069d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "'''\n",
    " Lasso Regression can handle multicollinearity in the input features, although its approach to dealing with \n",
    " multicollinearity is different from that of other regression techniques. \n",
    " \n",
    " Multicollinearity refers to the situation where two or more predictor variables in a regression model are highly \n",
    " correlated with each other, making it difficult to distinguish their individual effects on the target variable. \n",
    " \n",
    " Lasso Regression addresses multicollinearity through its feature selection mechanism, which tends to exclude less \n",
    " important variables, effectively focusing on a subset of relevant features.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e61ef-8145-4d1a-b4a5-06d39864aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "'''\n",
    "Choosing the optimal value of the regularization parameter (lambda or alpha) in Lasso Regression involves finding a\n",
    "balance between model complexity and predictive performance on new, unseen data. Cross-validation is a common \n",
    "technique used to determine the best lambda value by evaluating the model's performance on different subsets of the\n",
    "training data.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
