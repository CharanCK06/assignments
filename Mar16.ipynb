{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521ec0d-e357-42f0-a07a-6264621230f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "'''\n",
    "Overfitting: Overfitting occurs when a model performs well on the training data but fails to generalize to new, \n",
    "unseen data. \n",
    "    train-> model-> high accuracy -> LOW BIAS\n",
    "    test -> model-> low accuracy  -> HIGH VARIANCE\n",
    "Solution: Regularization , Feature Selection.\n",
    "\n",
    "Underfitting: Underfitting occurs when a model is too simple or lacks the capacity to capture the underlying \n",
    "patterns in the data.\n",
    "    train-> model-> low accuracy -> HIGH BIAS\n",
    "    test -> model-> low accuracy -> HIGH VARIANCE\n",
    "Solution: Increease Model Complexity , Feature Engineering.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0a868-4b9a-42f5-a85a-97ba14f19ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "'''\n",
    "a. Cross-validation: Use techniques like k-fold cross-validation to assess the model's performance on multiple \n",
    "subsets of the data and detect if it's overfitting.\n",
    "\n",
    "b. Regularization: Introduce regularization techniques such as L1 or L2 regularization, which add a penalty term\n",
    "to the loss function to discourage overly complex models.\n",
    "\n",
    "c. Feature selection: Identify and select the most relevant features or reduce the dimensionality of the data to \n",
    "prevent the model from learning noise or irrelevant patterns.\n",
    "\n",
    "d. Early stopping: Monitor the model's performance on a validation set during training and stop training when the \n",
    "performance starts to deteriorate, preventing it from over-optimizing the training data.\n",
    "\n",
    "e. Ensemble methods: Combine multiple models, such as through techniques like bagging or boosting, to reduce the \n",
    "impact of overfitting by leveraging the wisdom of the crowd.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba726f-40d7-4e96-89c0-dae2012f75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "'''\n",
    "Underfitting: Underfitting occurs when a model is too simple or lacks the capacity to capture the underlying \n",
    "patterns in the data.\n",
    "    train-> model-> low accuracy -> HIGH BIAS\n",
    "    test -> model-> low accuracy -> HIGH VARIANCE\n",
    "\n",
    "Scenarios where underfitting can occur:\n",
    "\n",
    "Inappropriate regularization: While regularization techniques like L1 or L2 regularization can help prevent \n",
    "overfitting, excessive regularization or too strong penalty terms can lead to underfitting. High regularization\n",
    "can overly constrain the model's flexibility, resulting in a simplified representation that does not capture \n",
    "the complexity of the data.\n",
    "\n",
    "Insufficient training data: If the amount of available training data is insufficient to capture the underlying \n",
    "patterns accurately, an underfitted model can result. With limited data, the model may not have enough examples\n",
    "to learn the relationships effectively.\n",
    "\n",
    "Insufficient model complexity: If the chosen model is too simple to represent the underlying patterns in the data,\n",
    "it can result in underfitting. For example, using a linear regression model to fit a highly non-linear relationship\n",
    "between variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e56ddc-4e1b-4ed0-9480-8f3db15f9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "'''\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between the \n",
    "bias and variance of a model and their impact on model performance.\n",
    "\n",
    "Bias:\n",
    "- It refers to the error introduced by approximating a real-world problem with a simplified model.\n",
    "- A high bias indicates that the model is not capturing the true underlying patterns in the data and is overly simplistic.\n",
    "\n",
    "Variance:\n",
    "- It refers to the amount by which the model's predictions vary for different training datasets. \n",
    "- It captures the sensitivity of the model to the randomness in the training data.\n",
    "-  A high variance indicates that the model is highly influenced by the specific examples in the training set and \n",
    "    tends to overfit by capturing noise or random fluctuations.\n",
    "    \n",
    "The goal in machine learning is to strike a balance between bias and variance to achieve optimal model performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c9182-8ece-4d79-9e31-25d5382b6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "'''\n",
    "Common methods for detecting overfitting and underfitting:\n",
    "\n",
    "i.Cross-validation: Use techniques like k-fold cross-validation to evaluate the model's performance on multiple \n",
    "    subsets of the data. If the model consistently performs well across all folds, it suggests that the model generalizes\n",
    "    well. However, if the model performs significantly better on the training folds than on the validation folds,\n",
    "    it indicates overfitting.\n",
    "ii.Validation curves: Vary a hyperparameter (e.g., regularization strength, learning rate) and plot the model's\n",
    "    performance on the training and validation sets. If the performance on the training set continues to improve \n",
    "    with increasing hyperparameter value while the performance on the validation set decreases, it suggests overfitting.\n",
    "    Conversely, if the performance on both sets is consistently low, it suggests underfitting.\n",
    "iii.Evaluation on a separate test set: Split the data into training and test sets. Train the model on the training set \n",
    "    and evaluate its performance on the test set. If the model performs significantly better on the training set than on\n",
    "    the test set, it indicates possible overfitting. Conversely, if the performance is poor on both the training and test \n",
    "    sets, it suggests underfitting.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75d301-ee28-48e5-a9d1-ee2982d82c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "'''\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between the \n",
    "bias and variance of a model and their impact on model performance.\n",
    "\n",
    "Bias:\n",
    "- It refers to the error introduced by approximating a real-world problem with a simplified model.\n",
    "- A high bias indicates that the model is not capturing the true underlying patterns in the data and is overly simplistic.\n",
    "\n",
    "Variance:\n",
    "- It refers to the amount by which the model's predictions vary for different training datasets. \n",
    "- It captures the sensitivity of the model to the randomness in the training data.\n",
    "-  A high variance indicates that the model is highly influenced by the specific examples in the training set and \n",
    "    tends to overfit by capturing noise or random fluctuations.\n",
    "\n",
    "Over-fitting:\n",
    "    train-> model-> high accuracy -> LOW BIAS\n",
    "    test -> model-> low accuracy  -> HIGH VARIANCE\n",
    "Under-fitting:\n",
    "    train-> model-> low accuracy -> HIGH BIAS\n",
    "    test -> model-> low accuracy -> HIGH VARIANCE\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c086f8-13a0-47d2-a2e3-ab7d9da7603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "'''\n",
    "\n",
    "Regularization in machine learning is a technique used to prevent overfitting by adding a penalty term\n",
    "to the loss function during model training. The penalty discourages the model from becoming too complex\n",
    "and helps in achieving better generalization performance.\n",
    "\n",
    "Techniques:\n",
    "i. L1(Lasso)\n",
    "ii. L2(Ridge)\n",
    "iii. Elastic net regularization.\n",
    "iv. Dropout\n",
    "v. Early stopping\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
