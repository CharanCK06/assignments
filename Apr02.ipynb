{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a8611-1ce1-46c5-84af-7c0bde91531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#01\n",
    "'''\n",
    "Grid Search Cross-Validation (GridSearchCV) is a technique used in machine learning for hyperparameter tuning,\n",
    "a process of finding the optimal hyperparameter values for a model. \n",
    "\n",
    "Hyperparameters are configuration settings that are not learned from the data but are set prior to training.\n",
    "\n",
    "GridSearchCV systematically searches through a predefined hyperparameter grid, evaluating the model's performance\n",
    "using cross-validation, and helps identify the best combination of hyperparameter values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67195f8c-63c2-4f0b-99ce-9e806ba25f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#02\n",
    "'''\n",
    "Grid Search Cross-Validation (GridSearchCV):\n",
    "    Search Strategy:\n",
    "    - Exhaustively searches through a predefined grid of hyperparameter values.\n",
    "    - Considers all possible combinations within the specified grid.\n",
    "    \n",
    "    Use Cases:\n",
    "    Suitable when the hyperparameter search space is relatively small and the computational resources are sufficient\n",
    "    to explore all combinations.\n",
    "    Often used when the user has a specific set of hyperparameter values in mind.\n",
    "\n",
    "Randomized Search Cross-Validation (RandomizedSearchCV):\n",
    "    Search Strategy:\n",
    "    - Randomly samples a fixed number of hyperparameter combinations from a distribution or predefined range.\n",
    "    - Does not exhaustively search all possible combinations.\n",
    "\n",
    "    Use Cases:\n",
    "    Suitable when the hyperparameter search space is large, and exploring all combinations is impractical due to \n",
    "    computational constraints.\n",
    "    Particularly useful when the user wants to efficiently sample from a broad range of hyperparameter values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6e488-76c2-4586-99d4-a94d7fc74568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#03\n",
    "'''\n",
    "=>Causes of Data Leakage:\n",
    "1.Including Future Information:\n",
    "Using information in the training data that would not be available at the time of prediction. \n",
    "For example, using target variable values that occur after the event you are trying to predict.\n",
    "\n",
    "2.Data Preprocessing Mistakes:\n",
    "Performing data preprocessing steps based on the entire dataset, including the test set, before \n",
    "splitting into training and test sets. This can introduce information from the test set into the training process.\n",
    "\n",
    "3.Target Leakage:\n",
    "Including features that are highly correlated with the target variable but were not known at the time of prediction.\n",
    "Example:Using a customer's future purchase behavior as a feature in a model predicting whether they will make a purchase.\n",
    "\n",
    "4.Data Contamination:\n",
    "Introducing external information into the training process, such as using data that contains information about the \n",
    "test set or using data that has been manipulated or generated based on test set information.\n",
    "\n",
    "=>Why Data Leakage is a Problem:\n",
    "1.Overestimated Model Performance:\n",
    "Data leakage can lead to overly optimistic estimates of a model's performance during training and evaluation,\n",
    "giving a false sense of its predictive power.\n",
    "\n",
    "2.Poor Generalization:\n",
    "Models trained with leaked information may not generalize well to new, unseen data. The model may appear to perform\n",
    "well on the training and validation sets but fail to make accurate predictions on real-world data.\n",
    "\n",
    "3.Ineffective Model Deployment:\n",
    "A model that performs well on leaked information might be deployed to make predictions in a real-world setting where \n",
    "the leaked information is not available, resulting in poor performance.\n",
    "\n",
    "4.Misleading Feature Importance:\n",
    "Features derived from leaked information may be mistakenly considered important by the model, leading to incorrect \n",
    "insights about the true relationships between features and the target variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ac725-c672-41d3-8b0c-d3a7a8a2b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#04\n",
    "'''\n",
    "How to Prevent Data Leakage:\n",
    "\n",
    "1.Strict Separation of Training and Test Sets:\n",
    "Ensure a clear separation between the data used for training and the data used for testing. Only use information\n",
    "available at the time of prediction in the training process.\n",
    "\n",
    "2.Avoid Future Information:\n",
    "Exclude any features or target variable values that are derived from information that occurs after the point in\n",
    "time being predicted.\n",
    "\n",
    "3.Awareness during Data Preprocessing:\n",
    "Be cautious when performing data preprocessing steps and ensure they are applied separately to the training and\n",
    "test sets. For example, use statistics calculated from the training set for normalization.\n",
    "\n",
    "4.Feature Engineering Prudently:\n",
    "When creating new features, make sure they are based only on information that would have been available at the \n",
    "time of prediction.\n",
    "\n",
    "5.Regularly Validate and Monitor:\n",
    "Regularly validate models on new data and monitor their performance over time to detect any signs of data leakage\n",
    "or model decay.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f380b-ff13-4eb8-8682-1caf5d4eeadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#05\n",
    "'''\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification model.\n",
    "It provides a summary of the model's predictions compared to the actual outcomes for different classes. \n",
    "The matrix has four entries: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "These values help in assessing the model's accuracy, precision, recall, and other performance metrics.\n",
    "\n",
    "components of a confusion matrix:\n",
    "\n",
    "True Positives (TP):\n",
    "Instances where the model correctly predicted the positive class.\n",
    "\n",
    "True Negatives (TN):\n",
    "Instances where the model correctly predicted the negative class.\n",
    "\n",
    "False Positives (FP):\n",
    "Instances where the model incorrectly predicted the positive class when the true class is negative (Type I error).\n",
    "\n",
    "False Negatives (FN):\n",
    "Instances where the model incorrectly predicted the negative class when the true class is positive (Type II error).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf6257-07a9-4bee-bae4-2780a38ce433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#06\n",
    "'''\n",
    "Precision is the ratio of true positives (correctly predicted positive instances) to the total \n",
    "number of instances predicted as positive (sum of true positives and false positives).\n",
    "\n",
    "Precision measures the accuracy of the positive predictions made by the model. \n",
    "It answers the question: \"Of all instances predicted as positive, how many were actually positive?\n",
    "\n",
    "High precision is desirable when minimizing false positives is crucial. For example, in medical diagnoses, \n",
    "a high precision ensures that when the model predicts a positive outcome, it is highly likely to be correct.\n",
    "\n",
    "Recall is the ratio of true positives to the total number of actual positive instances\n",
    "(sum of true positives and false negatives)\n",
    "\n",
    "Recall measures the model's ability to correctly capture all positive instances. \n",
    "It answers the question: \"Of all actual positive instances, how many were correctly predicted by the model?\"\n",
    "\n",
    "High recall is desirable when minimizing false negatives is crucial. For example, \n",
    "in fraud detection, a high recall ensures that the model identifies most of the fraudulent transactions.\n",
    "\n",
    "Precision and recall are often in tension with each other. Improving precision may lead to a decrease in \n",
    "recall and vice versa.\n",
    "The F1-score, which is the harmonic mean of precision and recall, provides a balanced measure that takes \n",
    "both metrics into account.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9181c3-34f1-4de7-8d72-00fd39439f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#07\n",
    "'''\n",
    "Interpreting a confusion matrix allows you to understand the types of errors your classification\n",
    "model is making and provides insights into its performance. \n",
    "\n",
    "Analyzing Error Types:\n",
    "False Positives (Type I Errors):\n",
    "    Implications:\n",
    "    - The model may be too aggressive in predicting the positive class.\n",
    "    - It might result in unnecessary actions or costs associated with false positives.\n",
    "    Example:\n",
    "    In a medical test, a false positive might lead to unnecessary medical procedures.\n",
    "\n",
    "False Negatives (Type II Errors):\n",
    "    Implications:\n",
    "    - The model may be too conservative or cautious in predicting the positive class.\n",
    "    - It might lead to missed opportunities or risks associated with false negatives.\n",
    "    Example:\n",
    "    -In a fraud detection system, a false negative might result in overlooking actual fraudulent\n",
    "    transactions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c579f7-7508-4f36-b5ea-7a0e6524eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#08\n",
    "'''\n",
    "Precision is the ratio of true positives (correctly predicted positive instances) to the total \n",
    "number of instances predicted as positive (sum of true positives and false positives).\n",
    "\n",
    "Precision measures the accuracy of the positive predictions made by the model. \n",
    "It answers the question: \"Of all instances predicted as positive, how many were actually positive?\n",
    "\n",
    "High precision is desirable when minimizing false positives is crucial. For example, in medical diagnoses, \n",
    "a high precision ensures that when the model predicts a positive outcome, it is highly likely to be correct.\n",
    "\n",
    "Recall is the ratio of true positives to the total number of actual positive instances\n",
    "(sum of true positives and false negatives)\n",
    "\n",
    "Recall measures the model's ability to correctly capture all positive instances. \n",
    "It answers the question: \"Of all actual positive instances, how many were correctly predicted by the model?\"\n",
    "\n",
    "High recall is desirable when minimizing false negatives is crucial. For example, \n",
    "in fraud detection, a high recall ensures that the model identifies most of the fraudulent transactions.\n",
    "\n",
    "Precision and recall are often in tension with each other. Improving precision may lead to a decrease in \n",
    "recall and vice versa.\n",
    "The F1-score, which is the harmonic mean of precision and recall, provides a balanced measure that takes \n",
    "both metrics into account.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab3a7a5-d54e-4f87-b4f1-67f590533b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#09\n",
    "'''\n",
    "Accuracy is a performance metric that measures the overall correctness of a model's predictions.\n",
    "It is the ratio of correctly predicted instances (both positive and negative) to the total number of instances.\n",
    "\n",
    "Relationship with Confusion Matrix Components:\n",
    "1.True Positives (TP):\n",
    "    TP contributes to accuracy because it represents instances that are correctly predicted as positive.\n",
    "\n",
    "2.True Negatives (TN):\n",
    "    TN contributes to accuracy because it represents instances that are correctly predicted as negative.\n",
    "\n",
    "3.False Positives (FP):\n",
    "    FP does not contribute to accuracy because these instances are incorrectly predicted as positive.\n",
    "    However, they are part of the denominator in the accuracy formula.\n",
    "\n",
    "4.False Negatives (FN):\n",
    "    FN does not contribute to accuracy because these instances are incorrectly predicted as negative. \n",
    "    However, they are part of the denominator in the accuracy formula.\n",
    "\n",
    "Implications:\n",
    "\n",
    "High Accuracy:\n",
    "    When a model has high accuracy, it means that a large proportion of instances, both positive \n",
    "    and negative, are correctly predicted.\n",
    "\n",
    "Low Accuracy:\n",
    "    When accuracy is low, it indicates that a significant number of instances are misclassified \n",
    "    (both false positives and false negatives).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d399fc-8b7b-4233-9324-cb0484fe28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "'''\n",
    "\n",
    "A confusion matrix can be a valuable tool for identifying potential biases or limitations in a \n",
    "machine learning model, especially in the context of classification tasks. By analyzing the distribution\n",
    "of predictions across different classes, you can gain insights into how the model is performing and \n",
    "\n",
    "whether it exhibits biases or limitations. Here are several ways to leverage a confusion matrix for this purpose:\n",
    "\n",
    "1. Class Imbalance:\n",
    "Issue:\n",
    "\n",
    "If the dataset has imbalanced classes, where one class significantly outnumbers the other, a model \n",
    "may achieve high accuracy by simply predicting the majority class.\n",
    "Use the Confusion Matrix to:\n",
    "\n",
    "Examine the distribution of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) across classes.\n",
    "Check if the model is disproportionately predicting the majority class while neglecting the minority class.\n",
    "2. Biased Predictions:\n",
    "Issue:\n",
    "\n",
    "A biased model may consistently favor one class over the other, leading to skewed predictions.\n",
    "Use the Confusion Matrix to:\n",
    "\n",
    "Look at the number of FP and FN for each class.\n",
    "Identify if the model tends to make more errors in predicting a specific class.\n",
    "3. False Positives and False Negatives:\n",
    "Issue:\n",
    "\n",
    "Understanding the types of errors the model makes (False Positives and False Negatives) is crucial for assessing its limitations.\n",
    "Use the Confusion Matrix to:\n",
    "\n",
    "Analyze the specific instances where the model fails (FP and FN).\n",
    "Investigate whether there are patterns or characteristics common to misclassified instances.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
