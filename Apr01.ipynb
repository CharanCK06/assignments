{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ef86c-cbb0-4d2b-9661-8120273248fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#01\n",
    "'''\n",
    "Linear Regression:\n",
    "    Linear regression is used for predicting a continuous outcome variable.The output is a continuous range of values.\n",
    "    Example:\n",
    "    Predicting house prices based on features such as size, number of bedrooms, and location.\n",
    "    \n",
    "Logistic Regression:\n",
    "    Logistic regression is used for predicting the probability of an event occurring.The output is a probability that falls \n",
    "    between 0 and 1.\n",
    "    Example:\n",
    "    Predicting whether a student will pass or fail an exam based on the number of hours spent studying. Here, the output is \n",
    "    a probability of     passing (1) or failing (0).\n",
    "    \n",
    "Scenario:\n",
    "    Suppose you are working on a medical diagnosis problem to predict whether a patient has a certain disease or not based on various \n",
    "    health parameters (e.g., blood pressure, cholesterol levels). The outcome variable is binary: the patient either has the disease (1)\n",
    "    or does not have the disease (0). Logistic regression would be suitable for modeling the probability of having the disease based on \n",
    "    the input features,as it naturally handles binary classification problems and provides probabilities that can be interpreted as the \n",
    "    likelihood of the event occurring.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ab7a6-3bbd-4312-a9e2-28c59cae636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#02\n",
    "'''\n",
    "In logistic regression, the cost function, often referred to as the logistic loss or cross-entropy loss, is used to measure the \n",
    "difference between the predicted probabilities and the actual labels. The goal during optimization is to minimize this cost function.\n",
    "\n",
    "Optimization:\n",
    "Gradient Descent is commonly used to minimize the cost function in logistic regression. The algorithm works by iteratively updating \n",
    "the model parameters in the opposite direction of the gradient of the cost function with respect to the parameters.\n",
    "\n",
    "The optimization process continues until the algorithm converges to a minimum, i.e., the parameter values where the cost function is\n",
    "minimized. There are variations of gradient descent, such as Stochastic Gradient Descent (SGD) and Mini-Batch Gradient Descent, that\n",
    "are commonly used for logistic regression optimization in practice.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929a771-0f88-4f89-b837-b9f71b0accf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#03\n",
    "'''\n",
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the cost function. In the \n",
    "context of logistic regression, regularization helps control the complexity of the model by discouraging overly complex models that \n",
    "may fit the training data too closely and not generalize well to new, unseen data.\n",
    "\n",
    "By adding the regularization term, the model is encouraged to keep the values of the parameters small. This effectively prevents the \n",
    "model from fitting the training data too closely and makes it more likely to generalize well to new, unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3e4fa-67af-4c2d-92b1-ce935f92f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#04\n",
    "'''\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a classification model, \n",
    "particularly binary classification like logistic regression. It illustrates the trade-off between the true positive rate (sensitivity \n",
    "or recall) and the false positive rate as the discrimination threshold is varied.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d884f6-7707-49e3-9b1f-25c244e4a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#05\n",
    "'''\n",
    "Feature selection is a crucial step in building machine learning models, including logistic regression. It involves choosing a subset of\n",
    "relevant features from the original set of features to improve model performance, reduce overfitting, and enhance interpretability.\n",
    "\n",
    "L1 regularization adds a penalty term to the logistic regression cost function that is proportional to the absolute values of the model \n",
    "parameters. This encourages sparsity in the parameter estimates, effectively setting some coefficients to zero. Features with non-zero \n",
    "coefficients are selected.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1dc3b-fd7a-4055-8a5a-2c2ea705a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#06\n",
    "'''\n",
    "Strategies for dealing with class imbalance in logistic regression:\n",
    "\n",
    "Undersampling: \n",
    "Randomly remove instances from the majority class to balance the class distribution. This approach can help reduce the dominance of the\n",
    "majority class but may lead to loss of information.\n",
    "\n",
    "Oversampling:\n",
    "Replicate instances from the minority class to balance the class distribution. Techniques like random oversampling and synthetic \n",
    "oversampling (e.g., SMOTE - Synthetic Minority Over-sampling Technique) generate synthetic instances to supplement the minority class.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0fd8ad-fe21-458b-9c8c-0e44c44bc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#07\n",
    "'''\n",
    "Regularization: Introduce regularization terms (L1 or L2) in the logistic regression cost function to penalize large coefficients and\n",
    "prevent overfitting.\n",
    "\n",
    "Feature Selection: Select a subset of relevant features to simplify the model and improve generalization.\n",
    "\n",
    "Identify and Handle Outliers:Use visualization tools (e.g., box plots) to identify outliers.Apply robust techniques or transformations\n",
    "to reduce the impact of outliers.\n",
    "\n",
    "Impute missing values using techniques like mean imputation, median imputation, or machine learning-based imputation.\n",
    "\n",
    "Convert categorical variables into dummy variables using techniques like one-hot encoding.\n",
    "\n",
    "Feature Scaling: Standardize or normalize numerical features for easier interpretation of coefficients.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
